{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'quiptools_cuda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 16\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (AutoModelForCausalLM, AutoTokenizer,\n\u001b[1;32m     12\u001b[0m                           PreTrainedTokenizerFast)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_attn_mask_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \\\n\u001b[1;32m     14\u001b[0m     _prepare_4d_causal_attention_mask\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[1;32m     18\u001b[0m parser \u001b[38;5;241m=\u001b[39m argparse\u001b[38;5;241m.\u001b[39mArgumentParser()\n\u001b[1;32m     19\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--seed\u001b[39m\u001b[38;5;124m'\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[0;32m~/vq-test/lib/utils/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfinetune\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_wrapper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/vq-test/lib/utils/data_utils.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Dataset\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m codebook\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatmul_had\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m matmul_hadU\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# transform a flat vector to a symmetric matrix\u001b[39;00m\n",
      "File \u001b[0;32m~/vq-test/lib/codebook/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mquiptools_cuda\u001b[39;00m\n\u001b[1;32m      4\u001b[0m torch\u001b[38;5;241m.\u001b[39mlibrary\u001b[38;5;241m.\u001b[39mdefine(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquip_lib::decode_matvec_e8p\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Tensor x, Tensor Qidxs, Tensor grid_packed_abs, int m, int n) -> Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mlibrary\u001b[38;5;241m.\u001b[39mimpl_abstract(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquip_lib::decode_matvec_e8p\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_matvec_e8p_abstract\u001b[39m(\n\u001b[1;32m      8\u001b[0m         x: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m      9\u001b[0m         Qidxs: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m     10\u001b[0m         grid_packed_abs: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m     11\u001b[0m         m: \u001b[38;5;28mint\u001b[39m, n: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'quiptools_cuda'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer,\n",
    "                          PreTrainedTokenizerFast)\n",
    "from transformers.modeling_attn_mask_utils import \\\n",
    "    _prepare_4d_causal_attention_mask\n",
    "\n",
    "from lib import utils\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--seed', default=0, type=int)\n",
    "parser.add_argument('--batch_size', default=2, type=int)\n",
    "parser.add_argument('--devset_size', default=256, type=int)\n",
    "parser.add_argument('--ctx_size', default=4096, type=int)\n",
    "parser.add_argument('--base_model',\n",
    "                    default='/data/common-weights-cy/Llama-3.2-1B',\n",
    "                    type=str)\n",
    "parser.add_argument('--save_path', default='hessians/llama3.2-1b', type=str)\n",
    "parser.add_argument('--scratch_path', default=None, type=str)\n",
    "parser.add_argument('--chunk_size', default=256, type=int)\n",
    "parser.add_argument('--async_copy_speed', default=-1, type=int)\n",
    "parser.add_argument('--act_save_rate', default=4, type=int)\n",
    "parser.add_argument('--save_activations', action='store_true')\n",
    "parser.add_argument('--sample_proc', default=4, type=int)\n",
    "\n",
    "\n",
    "def move_fn(in_q, async_copy_speed):\n",
    "    # async copy to avoid slow disk\n",
    "    while True:\n",
    "        item = in_q.get()\n",
    "        if item is None:\n",
    "            return\n",
    "        src, tgt = item\n",
    "        if async_copy_speed > 0:\n",
    "            os.system(f'rsync --bwlimit={async_copy_speed} {src} {tgt}')\n",
    "        else:\n",
    "            os.system(f'rsync {src} {tgt}')\n",
    "        os.system(f'rm {src}')\n",
    "        print(f'moved {src} to {tgt}')\n",
    "\n",
    "\n",
    "def forward_layer(layer, position_ids, attention_mask, bs, device, in_q,\n",
    "                  out_q):\n",
    "    torch.set_grad_enabled(False)\n",
    "    layer = layer.to(device)\n",
    "    position_ids = position_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    done_qkv = utils.register_H_hook(layer.self_attn.q_proj, device)\n",
    "    done_o = utils.register_H_hook(layer.self_attn.o_proj, device)\n",
    "    done_up = utils.register_H_hook(layer.mlp.up_proj, device)\n",
    "    done_down = utils.register_H_hook(layer.mlp.down_proj, device)\n",
    "\n",
    "    while True:\n",
    "        dev_emb = in_q.get()\n",
    "        if dev_emb is None:\n",
    "            layer = layer.cpu()\n",
    "            position_ids = position_ids.cpu()\n",
    "            attention_mask = attention_mask.cpu()\n",
    "            out_q.put({\n",
    "                'qkv': done_qkv(),\n",
    "                'o': done_o(),\n",
    "                'up': done_up(),\n",
    "                'down': done_down()\n",
    "            })\n",
    "            return\n",
    "\n",
    "        assert len(dev_emb) % bs == 0\n",
    "        for i in range(len(dev_emb) // bs):\n",
    "            dev_emb[i * bs:(i + 1) * bs] = layer(\n",
    "                dev_emb[i * bs:(i + 1) * bs].to(device),\n",
    "                position_ids=position_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                use_cache=False,\n",
    "                output_attentions=False)[0].cpu()\n",
    "\n",
    "\n",
    "def accumulate(in_q, move_q, ngpus, args, transformer_layer_index):\n",
    "    Hs = {}\n",
    "    mus = {}\n",
    "    cts = {}\n",
    "\n",
    "    for i in range(ngpus):\n",
    "        out = in_q.get()\n",
    "        if i == 0:\n",
    "            for key in out:\n",
    "                Hs[key] = torch.zeros(out[key][0].shape,\n",
    "                                      dtype=out[key][0].dtype)\n",
    "                mus[key] = torch.zeros(out[key][1].shape,\n",
    "                                       dtype=out[key][1].dtype)\n",
    "                cts[key] = 0\n",
    "        for key in out:\n",
    "            Hs[key].add_(out[key][0])\n",
    "            mus[key].add_(out[key][1])\n",
    "            cts[key] += out[key][2]\n",
    "\n",
    "    keys = list(Hs.keys())\n",
    "\n",
    "    for key in Hs:\n",
    "        mus[key].div_(cts[key])\n",
    "        Hs[key].div_(cts[key])\n",
    "        Hs[key].addmm_(-mus[key].unsqueeze(-1), mus[key].unsqueeze(0))\n",
    "        save_path = f\"{args.scratch_path}/{transformer_layer_index}_{key}.pt\" if args.scratch_path is not None else f\"{args.save_path}/{transformer_layer_index}_{key}.pt\"\n",
    "        torch.save(\n",
    "            {\n",
    "                'flatH': utils.sym_to_flat(Hs[key].to(torch.float32)),\n",
    "                'mu': mus[key].to(torch.float32),\n",
    "                'n': Hs[key].shape[0],\n",
    "                'ct': cts[key]\n",
    "            }, save_path)\n",
    "        if args.scratch_path is not None:\n",
    "            move_q.put(\n",
    "                (f\"{args.scratch_path}/{transformer_layer_index}_{key}.pt\",\n",
    "                 f\"{args.save_path}/{transformer_layer_index}_{key}.pt\"))\n",
    "\n",
    "    del Hs, mus, cts, out\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    print(\"loading model...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(args.base_model,\n",
    "                                                 torch_dtype=\"auto\",\n",
    "                                                 low_cpu_mem_usage=True)\n",
    "    print(\"loaded model!\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.base_model, use_fast=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    if os.path.isfile(f\"{args.save_path}/dev_activations.pt\"):\n",
    "        print(\"loading cached dataset...\")\n",
    "        loaded_dev_activations = torch.load(\n",
    "            f\"{args.save_path}/dev_activations.pt\")\n",
    "        after_layer = loaded_dev_activations['after_layer']\n",
    "        dev_emb = loaded_dev_activations['dev_emb']\n",
    "        print(\n",
    "            f\"loaded cached dataset from {loaded_dev_activations['timestamp']}\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"loading dataset...\")\n",
    "        devset = utils.sample_rp1t(tokenizer,\n",
    "                                   args.devset_size,\n",
    "                                   args.ctx_size,\n",
    "                                   nproc=args.sample_proc)\n",
    "        dev_emb = model.model.embed_tokens(devset)\n",
    "        after_layer = -1\n",
    "        print(\"loaded dataset!\")\n",
    "\n",
    "    print(f\"dev_emb dtype: {dev_emb.dtype}\")\n",
    "    dev_emb.share_memory_()\n",
    "\n",
    "    position_ids = torch.arange(args.ctx_size, dtype=torch.int64)[None, :] + \\\n",
    "        torch.zeros(args.batch_size, args.ctx_size, dtype=torch.int64)\n",
    "    if hasattr(model.config, 'sliding_window'):\n",
    "        attention_mask = _prepare_4d_causal_attention_mask(\n",
    "            None, (args.batch_size, args.ctx_size),\n",
    "            dev_emb[0:args.batch_size],\n",
    "            0,\n",
    "            sliding_window=model.config.sliding_window)\n",
    "    else:\n",
    "        attention_mask = _prepare_4d_causal_attention_mask(\n",
    "            None, (args.batch_size, args.ctx_size),\n",
    "            dev_emb[0:args.batch_size], 0)\n",
    "\n",
    "    if args.scratch_path is not None:\n",
    "        move_q = mp.Queue()\n",
    "        move_p = mp.Process(target=move_fn,\n",
    "                            args=(move_q, args.async_copy_speed))\n",
    "        move_p.start()\n",
    "    else:\n",
    "        move_q = None\n",
    "\n",
    "    for transformer_layer_index in range(len(model.model.layers)):\n",
    "        if (transformer_layer_index <= after_layer):\n",
    "            print(\n",
    "                f\"skipping layer {transformer_layer_index} because it is before cached activations at layer {after_layer}\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        transformer_layer = model.model.layers[transformer_layer_index]\n",
    "        # check that there are four layers, as expected\n",
    "        assert (len([\n",
    "            m for m in transformer_layer.modules()\n",
    "            if isinstance(m, torch.nn.Linear)\n",
    "        ]) == 7)\n",
    "\n",
    "        chunk_size = min(args.chunk_size, len(dev_emb))\n",
    "        ngpus = min(torch.cuda.device_count(), len(dev_emb) // chunk_size)\n",
    "\n",
    "        manager = mp.get_context('spawn').Manager()\n",
    "        in_q = manager.Queue()\n",
    "        out_q = manager.Queue()\n",
    "\n",
    "        accumulate_proc = mp.Process(target=accumulate,\n",
    "                                     args=(out_q, move_q, ngpus, args,\n",
    "                                           transformer_layer_index))\n",
    "        accumulate_proc.start()\n",
    "\n",
    "        forward_procs = []\n",
    "        for i in range(ngpus):\n",
    "            p = mp.Process(target=forward_layer,\n",
    "                           args=(transformer_layer, position_ids,\n",
    "                                 attention_mask, args.batch_size, i, in_q,\n",
    "                                 out_q))\n",
    "            p.start()\n",
    "            forward_procs.append(p)\n",
    "\n",
    "        assert len(\n",
    "            dev_emb\n",
    "        ) % args.batch_size == 0 and chunk_size % args.batch_size == 0\n",
    "        i = 0\n",
    "        while i < len(dev_emb):\n",
    "            next = min(i + chunk_size, len(dev_emb))\n",
    "            in_q.put(dev_emb[i:next])\n",
    "            i = next\n",
    "\n",
    "        for i in range(ngpus):\n",
    "            in_q.put(None)\n",
    "\n",
    "        for p in forward_procs:\n",
    "            p.join()\n",
    "\n",
    "        accumulate_proc.join()\n",
    "\n",
    "        transformer_layer.cpu()\n",
    "        model.model.layers[transformer_layer_index] = None\n",
    "        utils.clean()\n",
    "\n",
    "        if args.save_activations and (\n",
    "                transformer_layer_index % args.act_save_rate == 0 or \\\n",
    "                transformer_layer_index == len(model.model.layers) - 1):\n",
    "            if args.scratch_path is not None:\n",
    "                if os.path.exists(f'{args.scratch_path}/dev_activations.pt'):\n",
    "                    print('not saving layer since disk is too slow')\n",
    "                else:\n",
    "                    torch.save(\n",
    "                        {\n",
    "                            'dev_emb': dev_emb,\n",
    "                            'after_layer': transformer_layer_index,\n",
    "                            'timestamp': str(datetime.datetime.now())\n",
    "                        }, f'{args.scratch_path}/dev_activations.pt')\n",
    "                    move_q.put((f'{args.scratch_path}/dev_activations.pt',\n",
    "                                f'{args.save_path}/dev_activations.pt'))\n",
    "            else:\n",
    "                torch.save(\n",
    "                    {\n",
    "                        'dev_emb': dev_emb,\n",
    "                        'after_layer': transformer_layer_index,\n",
    "                        'timestamp': str(datetime.datetime.now())\n",
    "                    }, f'{args.save_path}/dev_activations.pt')\n",
    "\n",
    "        print(f\"done processing layer {transformer_layer_index}\")\n",
    "\n",
    "    if args.scratch_path is not None:\n",
    "        move_q.put(None)\n",
    "        move_p.join()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mp.set_start_method('spawn')\n",
    "    torch.set_grad_enabled(False)\n",
    "    args = parser.parse_args()\n",
    "    torch.manual_seed(args.seed)\n",
    "    random.seed(args.seed)\n",
    "    numpy.random.seed(args.seed)\n",
    "    os.makedirs(args.save_path, exist_ok=True)\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 155,  205,  255],\n",
      "        [ 507,  585,  663],\n",
      "        [1011, 1197, 1383]])\n",
      "tensor([[ 155,  205,  255],\n",
      "        [ 507,  585,  663],\n",
      "        [1011, 1197, 1383]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a=torch.tensor([[1,2,3],\n",
    "                [4,5,6],\n",
    "                [7,8,9]])\n",
    "c=torch.tensor([[18,29,3],\n",
    "                [4,5,69],\n",
    "                [7,83,96]])\n",
    "\n",
    "b=torch.tensor([2,1,0])\n",
    "\n",
    "tensor1=torch.index_select(a,0,b)\n",
    "tensor2=torch.index_select(c,1,b)\n",
    "\n",
    "print(c@a)\n",
    "print(tensor2@tensor1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "排序后的张量: tensor([ 2,  5,  8, 10])\n",
      "排序前的索引: tensor([3, 1, 2, 0])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个一维张量\n",
    "tensor = torch.tensor([10, 5, 8, 2])\n",
    "\n",
    "# 对张量进行排序，并保留排序前的索引\n",
    "sorted_tensor, indices = torch.sort(tensor)\n",
    "\n",
    "print(\"排序后的张量:\", sorted_tensor)\n",
    "print(\"排序前的索引:\", indices)\n",
    "print(type(indices))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
